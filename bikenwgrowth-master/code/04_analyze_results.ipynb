{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Analysis of bicycle network results\n",
    "## Project: Growing Urban Bicycle Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the existing infrastructure, the results from 03_poi_based_generation and calculates/analyzes a number of measures:\n",
    "* cost (length)\n",
    "* coverage  \n",
    "* directness  \n",
    "* efficiency\n",
    "* overlap with existing networks\n",
    "\n",
    "Contact: Michael Szell (michael.szell@gmail.com)  \n",
    "Created: 2020-07-08  \n",
    "Last modified: 2021-11-22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "debug = False # If True, will produce plots and/or verbose output to double-check\n",
    "rerun_existing = False # If True, will re-run the costly analysis of existing infra even if files already exist.\n",
    "%run -i \"../parameters/parameters.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PATH.\n",
      "\n",
      "Setup finished.\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.2\n",
      "IPython version      : 8.5.0\n",
      "\n",
      "Compiler    : Clang 9.0.1 \n",
      "OS          : Darwin\n",
      "Release     : 18.7.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 91b7c308c5580dfbed9ad0603785edcccf3d2214\n",
      "\n",
      "igraph    : 0.9.1\n",
      "sklearn   : 1.1.2\n",
      "geojson   : 2.5.0\n",
      "geopandas : 0.11.1\n",
      "csv       : 1.0\n",
      "shapely   : 1.8.4\n",
      "networkx  : 2.8.6\n",
      "watermark : 2.3.1\n",
      "osgeo     : 3.2.1\n",
      "pandas    : 1.4.4\n",
      "fiona     : 1.8.21\n",
      "pyproj    : 3.4.0\n",
      "numpy     : 1.23.3\n",
      "sys       : 3.8.2 | packaged by conda-forge | (default, Apr 24 2020, 07:56:27) \n",
      "[Clang 9.0.1 ]\n",
      "osmnx     : 0.16.2\n",
      "matplotlib: 3.6.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run -i path.py\n",
    "#%run -i setup.py\n",
    "%run -i setupCPH.py\n",
    "if not debug: # Only do this if sure the code is bug-free!\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded functions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run -i functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join attributes on all networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../bikenwgrowth_external/data/copenhagen/bikedata/mean_length_attr.pkl\", 'rb') as f:\n",
    "    unassigned_length_attr = pickle.load(f)\n",
    "with open(\"../../bikenwgrowth_external/data/copenhagen/bikedata/mean_pop_den_attr.pkl\", 'rb') as f:\n",
    "    unassigned_pop_den_attr = pickle.load(f)\n",
    "with open(\"../../bikenwgrowth_external/data/copenhagen/bikedata/mean_bcount_attr.pkl\", 'rb') as q:\n",
    "    unassigned_bcount_attr = pickle.load(q)\n",
    "with open(\"../../bikenwgrowth_external/data/copenhagen/bikedata/mean_bcount_pop_den_att.pkl\", 'rb') as q:\n",
    "    unassigned_bcount_pop_den_attr = pickle.load(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join new attributes to all networks on geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"../../bikenwgrowth_external/data/copenhagen/copenhagen_carall_edges.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"../../bikenwgrowth_external/data/copenhagen/\")\n",
    "\n",
    "carall_edges = pd.read_csv(\"../../bikenwgrowth_external/data/copenhagen/copenhagen_carall_edges.csv\")\n",
    "\n",
    "for network in networktypesdata:\n",
    "    print(network)\n",
    "    \n",
    "    with zipfile.ZipFile(\"../../bikenwgrowth_external/data/copenhagen/copenhagen_\"+network+\"_edges.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"../../bikenwgrowth_external/data/copenhagen/\")\n",
    "\n",
    "    biketrack_edges = pd.read_csv(\"../../bikenwgrowth_external/data/copenhagen/copenhagen_\"+network+\"_edges.csv\")\n",
    "\n",
    "\n",
    "    #the initial edges are joined with the new generalised counts\n",
    "    result = pd.merge(biketrack_edges, carall_edges[['geometry','length_attr','bcount_attr','pop_den_attr','bcount_pop_den_attr']],how = 'left',on='geometry')\n",
    "    #left_on=['u','v'], right_on = ['u','v']\n",
    "\n",
    "    for i in range(len(result)):\n",
    "        length_attr = result['length_attr'].iloc[i]\n",
    "        bcount_attr = result['bcount_attr'].iloc[i]\n",
    "        pop_den_attr = result['pop_den_attr'].iloc[i]\n",
    "        bcount_pop_den_attr = result['bcount_pop_den_attr'].iloc[i]\n",
    "        #if num is NaN\n",
    "        if length_attr != length_attr:\n",
    "            result['length_attr'].iloc[i]= unassigned_length_attr\n",
    "        if bcount_attr != bcount_attr:\n",
    "            result['bcount_attr'].iloc[i]= unassigned_bcount_attr\n",
    "        if pop_den_attr != pop_den_attr:\n",
    "            result['pop_den_attr'].iloc[i]= unassigned_pop_den_attr\n",
    "        if bcount_pop_den_attr != bcount_pop_den_attr:\n",
    "            result['bcount_pop_den_attr'].iloc[i]= unassigned_bcount_pop_den_attr\n",
    "            \n",
    "    result.to_csv(\"../../bikenwgrowth_external/data/copenhagen/copenhagen_\"+network+\"_edges.csv\")\n",
    "    compress_file(\"../../bikenwgrowth_external/data/copenhagen/\",\"copenhagen_\"+network+\"_edges\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze existing infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for placeid, placeinfo in tqdm(cities.items(), desc = \"Cities\"):\n",
    "    print(placeid + \": Analyzing existing infrastructure.\")\n",
    "    \n",
    "    # output_place is one static file for the existing city. This can be compared to the generated infrastructure.\n",
    "    # Make a check if this file was already generated - it only needs to be done once. If not, generate it:\n",
    "    filename = placeid + \"_existing.csv\"\n",
    "    if rerun_existing or not os.path.isfile(PATH[\"results\"] + placeid + \"/\" + filename):\n",
    "        empty_metrics = {\n",
    "                         \"length\":0,\n",
    "                         \"length_lcc\":0,\n",
    "                         \"coverage\": 0,\n",
    "                         \"directness\": 0,\n",
    "                         \"directness_lcc\": 0,\n",
    "                         \"poi_coverage\": 0,\n",
    "                         \"components\": 0,\n",
    "                         \"efficiency_global\": 0,\n",
    "                         #\"efficiency_local\": 0,\n",
    "                         \"efficiency_global_routed\": 0,\n",
    "                         #\"efficiency_local_routed\": 0,\n",
    "                         \"directness_lcc_linkwise\": 0,\n",
    "                         \"directness_all_linkwise\": 0\n",
    "                        }\n",
    "        output_place = {}\n",
    "        for networktype in networktypes:\n",
    "            output_place[networktype] = copy.deepcopy(empty_metrics)\n",
    "\n",
    "        # Analyze all networks     \n",
    "        Gs = {}\n",
    "        for networktype in networktypes:\n",
    "            if networktype != \"biketrack_onstreet\" and networktype != \"bikeable_offstreet\":\n",
    "                Gs[networktype] = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, networktype)\n",
    "                Gs[networktype + \"_simplified\"] = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, networktype + \"_simplified\")\n",
    "                #Gs[networktype] = csv_to_ig_custom(PATH[\"data\"] + placeid + \"/\", placeid, networktype)\n",
    "                #Gs[networktype + \"_simplified\"] = csv_to_ig_custom(PATH[\"data\"] + placeid + \"/\", placeid, networktype + \"_simplified\")\n",
    "            elif networktype == \"biketrack_onstreet\":\n",
    "                Gs[networktype] = intersect_igraphs(Gs[\"biketrack\"], Gs[\"carall\"])\n",
    "                Gs[networktype + \"_simplified\"] = intersect_igraphs(Gs[\"biketrack_simplified\"], Gs[\"carall_simplified\"])\n",
    "            elif networktype == \"bikeable_offstreet\":\n",
    "                G_temp = copy.deepcopy(Gs[\"bikeable\"])\n",
    "                delete_overlaps(G_temp, Gs[\"carall\"])\n",
    "                Gs[networktype] = G_temp\n",
    "                G_temp = copy.deepcopy(Gs[\"bikeable_simplified\"])\n",
    "                delete_overlaps(G_temp, Gs[\"carall_simplified\"])\n",
    "                Gs[networktype + \"_simplified\"] = G_temp\n",
    "        \n",
    "        with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_' + poi_source + '_nnidscarall.csv') as f:\n",
    "            nnids = [int(line.rstrip()) for line in f]\n",
    "\n",
    "            \n",
    "        covs = {}\n",
    "        for networktype in tqdm(networktypes, desc = \"Networks\", leave = False):\n",
    "            if debug: print(placeid + \": Analyzing results: \" + networktype)\n",
    "            metrics, cov = calculate_metrics(Gs[networktype], Gs[networktype + \"_simplified\"], Gs['carall'], nnids, empty_metrics, buffer_walk, numnodepairs, debug)\n",
    "            #metrics, cov = calculate_metrics_custom(Gs[networktype], Gs[networktype + \"_simplified\"], Gs['carall'], nnids, empty_metrics, buffer_walk, numnodepairs, debug)\n",
    "            \n",
    "            for key, val in metrics.items():\n",
    "                output_place[networktype][key] = val\n",
    "            covs[networktype] = cov\n",
    "        # Save the covers\n",
    "        write_result(covs, \"pickle\", placeid, \"\", \"\", \"existing_covers.pickle\")\n",
    "        \n",
    "        # Write to CSV\n",
    "        write_result(output_place, \"dictnested\", placeid, \"\", \"\", \"existing.csv\", empty_metrics)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for placeid, placeinfo in tqdm(cities.items(), desc = \"Cities\"):\n",
    "    print(placeid + \": Analyzing existing infrastructure.\")\n",
    "    \n",
    "    # output_place is one static file for the existing city. This can be compared to the generated infrastructure.\n",
    "    # Make a check if this file was already generated - it only needs to be done once. If not, generate it:\n",
    "    for attr in attrlist:\n",
    "        filename = placeid + \"_\"+attr +\"_existing.csv\"\n",
    "        if rerun_existing or not os.path.isfile(PATH[\"results\"] + placeid + \"/\" + filename):\n",
    "            empty_metrics = {\n",
    "                             \"length\":0,\n",
    "                             \"length_lcc\":0,\n",
    "                             \"coverage\": 0,\n",
    "                             \"directness\": 0,\n",
    "                             \"directness_lcc\": 0,\n",
    "                             #\"poi_coverage\": 0,\n",
    "                             #\"components\": 0,\n",
    "                             #\"efficiency_global\": 0,\n",
    "                             #\"efficiency_local\": 0,\n",
    "                             #\"efficiency_global_routed\": 0,\n",
    "                             #\"efficiency_local_routed\": 0,\n",
    "                             #\"directness_lcc_linkwise\": 0,\n",
    "                             #\"directness_all_linkwise\": 0\n",
    "                            }\n",
    "            output_place = {}\n",
    "            for networktype in networktypes:\n",
    "                output_place[networktype] = copy.deepcopy(empty_metrics)\n",
    "\n",
    "            # Analyze all networks     \n",
    "            Gs = {}\n",
    "            for networktype in networktypes:\n",
    "                if networktype != \"biketrack_onstreet\" and networktype != \"bikeable_offstreet\":\n",
    "                    graph = csv_to_ig_custom(PATH[\"data\"] + placeid + \"/\", placeid, networktype,attr)\n",
    "                    Gs[networktype] = graph\n",
    "                    graph_simplified = simplify_ig(graph)\n",
    "                    Gs[networktype + \"_simplified\"] = graph_simplified\n",
    "                    #Gs[networktype] = csv_to_ig_custom(PATH[\"data\"] + placeid + \"/\", placeid, networktype)\n",
    "                    #Gs[networktype + \"_simplified\"] = csv_to_ig_custom(PATH[\"data\"] + placeid + \"/\", placeid, networktype + \"_simplified\")\n",
    "                elif networktype == \"biketrack_onstreet\":\n",
    "                    graph_biketrack = Gs[\"biketrack\"]\n",
    "                    graph_carall =Gs[\"carall\"]\n",
    "                    Gs[networktype] = intersect_igraphs(graph_biketrack, graph_carall)\n",
    "                    Gs[networktype + \"_simplified\"] = intersect_igraphs(simplify_ig(graph_biketrack), simplify_ig(graph_carall))\n",
    "                elif networktype == \"bikeable_offstreet\":\n",
    "                    G_temp = copy.deepcopy(Gs[\"bikeable\"])\n",
    "                    delete_overlaps(G_temp, Gs[\"carall\"])\n",
    "                    Gs[networktype] = G_temp\n",
    "                    G_temp = copy.deepcopy(simplify_ig(Gs[\"bikeable\"]))\n",
    "                    delete_overlaps(G_temp, Gs[\"carall_simplified\"])\n",
    "                    Gs[networktype + \"_simplified\"] = G_temp\n",
    "\n",
    "            with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_' + poi_source + '_nnidscarall.csv') as f:\n",
    "                nnids = [int(line.rstrip()) for line in f]\n",
    "            \n",
    "            \n",
    "            \n",
    "            covs = {}\n",
    "            for networktype in tqdm(networktypes, desc = \"Networks\", leave = False):\n",
    "                if debug: print(placeid + \": Analyzing results: \" + networktype)\n",
    "                metrics, cov = calculate_metrics(Gs[networktype], Gs[networktype + \"_simplified\"], Gs['carall'], nnids, empty_metrics, buffer_walk, numnodepairs, debug)\n",
    "                #metrics, cov = calculate_metrics_custom(Gs[networktype], Gs[networktype + \"_simplified\"], Gs['carall'], nnids, empty_metrics, buffer_walk, numnodepairs, debug)\n",
    "\n",
    "                for key, val in metrics.items():\n",
    "                    output_place[networktype][key] = val\n",
    "                covs[networktype] = cov\n",
    "            # Save the covers\n",
    "            write_result(covs, \"pickle\", placeid, \"\", \"\",attr+\"_\"+\"existing_covers.pickle\")\n",
    "\n",
    "            # Write to CSV\n",
    "            write_result(output_place, \"dictnested\", placeid, \"\", \"\", attr+\"_\"+\"existing.csv\", empty_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze POI based results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for placeid, placeinfo in tqdm(cities.items(), desc = \"Cities\"):\n",
    "    print(placeid + \": Analyzing results\")\n",
    "\n",
    "    # Load networks\n",
    "    G_carall = csv_to_ig_custom(PATH[\"data\"] + placeid + \"/\", placeid, 'carall','length')\n",
    "    #G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "    Gexisting = {}\n",
    "    for networktype in [\"biketrack\", \"bikeable\"]:\n",
    "        Gexisting[networktype] = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, networktype)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Load POIs\n",
    "    with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_' + poi_source + '_nnidscarall.csv') as f:\n",
    "        nnids = [int(line.rstrip()) for line in f]\n",
    "            \n",
    "    # Load results\n",
    "    filename = placeid + '_poi_' + poi_source + \"_\" + prune_measure\n",
    "    resultfile = open(PATH[\"results\"] + placeid + \"/\" + filename + \".pickle\",'rb')\n",
    "    res = pickle.load(resultfile)\n",
    "    resultfile.close()\n",
    "    if debug: pp.pprint(res)\n",
    "         \n",
    "    # Calculate\n",
    "    # output contains lists for all the prune_quantile values of the corresponding results\n",
    "    #output, covs = calculate_metrics_additively_custom(res[\"GTs\"], res[\"GT_abstracts\"], res[\"prune_quantiles\"], G_carall, nnids, buffer_walk, numnodepairs, debug, True, Gexisting)\n",
    "    #output_MST, cov_MST = calculate_metrics_custom(res[\"MST\"], res[\"MST_abstract\"], G_carall, nnids, output, buffer_walk, numnodepairs, debug, True, ig.Graph(), Polygon(), False, Gexisting)\n",
    "    \n",
    "    output, covs = calculate_metrics_additively(res[\"GTs\"], res[\"GT_abstracts\"], res[\"prune_quantiles\"], G_carall, nnids, buffer_walk, numnodepairs, debug, True, Gexisting)\n",
    "    output_MST, cov_MST = calculate_metrics(res[\"MST\"], res[\"MST_abstract\"], G_carall, nnids, output, buffer_walk, numnodepairs, debug, True, ig.Graph(), Polygon(), False, Gexisting)\n",
    "        \n",
    "    # Save the covers\n",
    "    write_result(covs, \"pickle\", placeid, poi_source, prune_measure, \"_covers.pickle\")\n",
    "#     write_result(covs_carminusbike, \"pickle\", placeid, poi_source, prune_measure, \"_covers_carminusbike.pickle\")\n",
    "    write_result(cov_MST, \"pickle\", placeid, poi_source, prune_measure, \"_cover_mst.pickle\")\n",
    "        \n",
    "    # Write to CSV\n",
    "    write_result(output, \"dict\", placeid, poi_source, prune_measure, \".csv\")\n",
    "#     write_result(output_carminusbike, \"dict\", placeid, poi_source, prune_measure, \"_carminusbike.csv\")\n",
    "#     write_result(output_carconstrictedbike, \"dict\", placeid, poi_source, prune_measure, \"_carconstrictedbike.csv\")\n",
    "    write_result(output_MST, \"dict\", placeid, poi_source, \"\", \"mst.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for placeid, placeinfo in tqdm(cities.items(), desc = \"Cities\"):\n",
    "    print(placeid + \": Analyzing results\")\n",
    "    for attr in attrlist:\n",
    "        print(\"attr: \" + attr)\n",
    "    # Load networks\n",
    "        G_carall = csv_to_ig_custom(PATH[\"data\"] + placeid + \"/\", placeid, 'carall',attr)\n",
    "        #G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "        Gexisting = {}\n",
    "        for networktype in [\"biketrack\", \"bikeable\"]:\n",
    "            Gexisting[networktype] = csv_to_ig_custom(PATH[\"data\"] + placeid + \"/\", placeid, networktype, attr)\n",
    "\n",
    "\n",
    "\n",
    "        # Load POIs\n",
    "        with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_' + poi_source + '_nnidscarall.csv') as f:\n",
    "            nnids = [int(line.rstrip()) for line in f]\n",
    "\n",
    "        # Load results\n",
    "        filename = placeid + '_poi_' + poi_source + \"_\" + prune_measure + \"_\"+ attr\n",
    "        resultfile = open(PATH[\"results\"] + placeid + \"/\" + filename  +\".pickle\",'rb')\n",
    "        res = pickle.load(resultfile)\n",
    "        resultfile.close()\n",
    "        if debug: pp.pprint(res)\n",
    "        print(1)\n",
    "        # Calculate\n",
    "        # output contains lists for all the prune_quantile values of the corresponding results\n",
    "        #output, covs = calculate_metrics_additively_custom(res[\"GTs\"], res[\"GT_abstracts\"], res[\"prune_quantiles\"], G_carall, nnids, buffer_walk, numnodepairs, debug, True, Gexisting)\n",
    "        #output_MST, cov_MST = calculate_metrics_custom(res[\"MST\"], res[\"MST_abstract\"], G_carall, nnids, output, buffer_walk, numnodepairs, debug, True, ig.Graph(), Polygon(), False, Gexisting)\n",
    "\n",
    "        output, covs = calculate_metrics_additively(res[\"GTs\"], res[\"GT_abstracts\"], res[\"prune_quantiles\"], G_carall, nnids, buffer_walk, numnodepairs, debug, True, Gexisting)\n",
    "        output_MST, cov_MST = calculate_metrics(res[\"MST\"], res[\"MST_abstract\"], G_carall, nnids, output, buffer_walk, numnodepairs, debug, True, ig.Graph(), Polygon(), False, Gexisting)\n",
    "\n",
    "        # Save the covers\n",
    "        write_result(covs, \"pickle\", placeid, poi_source, prune_measure,  \"_\"+ attr+\"_covers.pickle\")\n",
    "    #     write_result(covs_carminusbike, \"pickle\", placeid, poi_source, prune_measure, \"_covers_carminusbike.pickle\")\n",
    "        write_result(cov_MST, \"pickle\", placeid, poi_source, prune_measure, \"_\"+ attr+\"_cover_mst.pickle\")\n",
    "\n",
    "        # Write to CSV\n",
    "        write_result(output, \"dict\", placeid, poi_source, prune_measure, \"_\"+ attr+\".csv\")\n",
    "    #     write_result(output_carminusbike, \"dict\", placeid, poi_source, prune_measure, \"_carminusbike.csv\")\n",
    "    #     write_result(output_carconstrictedbike, \"dict\", placeid, poi_source, prune_measure, \"_carconstrictedbike.csv\")\n",
    "        write_result(output_MST, \"dict\", placeid, poi_source, \"\", \"_\"+ attr+\"mst.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f4cc1a69624f0990ad73a78b65c937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cities:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copenhagen: Analyzing results\n",
      "attr: length\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d2979405134051941edfc398ec5821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Bicycle networks:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for placeid, placeinfo in tqdm(cities.items(), desc = \"Cities\"):\n",
    "    print(placeid + \": Analyzing results\")\n",
    "\n",
    "# Load networks\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    #filename = placeid + '_poi_' + poi_source + \"_\" + prune_measure\n",
    "    #resultfile = open(PATH[\"results\"] + placeid + \"/\" + filename + \".pickle\",'rb')\n",
    "    #res = pickle.load(resultfile)\n",
    "    #resultfile.close()\n",
    "    #if debug: pp.pprint(res)\n",
    "    #output, covs = calculate_metrics_additively(res[\"GTs\"], res[\"GT_abstracts\"], res[\"prune_quantiles\"], G_carall, nnids, buffer_walk, numnodepairs, debug, True, Gexisting)\n",
    "    #output_MST, cov_MST = calculate_metrics(res[\"MST\"], res[\"MST_abstract\"], G_carall, nnids, output, buffer_walk, numnodepairs, debug, True, ig.Graph(), Polygon(), False, Gexisting)\n",
    "    #write_result(covs, \"pickle\", placeid, poi_source, prune_measure, \"_covers.pickle\")\n",
    "    #write_result(cov_MST, \"pickle\", placeid, poi_source, prune_measure, \"_cover_mst.pickle\")\n",
    "    #write_result(output, \"dict\", placeid, poi_source, prune_measure, \".csv\")\n",
    "    #write_result(output_MST, \"dict\", placeid, poi_source, \"\", \"_mst.csv\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for attr in attrlist:\n",
    "        print(\"attr: \" + attr)\n",
    "        \n",
    "        G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "        #G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "        Gexisting = {}\n",
    "        for networktype in [\"biketrack\", \"bikeable\"]:\n",
    "            Gexisting[networktype] = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, networktype)\n",
    "\n",
    "\n",
    "\n",
    "    # Load POIs\n",
    "        with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_' + poi_source + '_nnidscarall.csv') as f:\n",
    "            nnids = [int(line.rstrip()) for line in f]\n",
    "    # Load results\n",
    "        filename = placeid + '_poi_' + poi_source + \"_\" + prune_measure + \"_\"+ attr\n",
    "        resultfile = open(PATH[\"results\"] + placeid + \"/\" + filename  +\".pickle\",'rb')\n",
    "        res = pickle.load(resultfile)\n",
    "        resultfile.close()\n",
    "        if debug: pp.pprint(res)\n",
    "        # Calculate\n",
    "        # output contains lists for all the prune_quantile values of the corresponding results\n",
    "        #output, covs = calculate_metrics_additively_custom(res[\"GTs\"], res[\"GT_abstracts\"], res[\"prune_quantiles\"], G_carall, nnids, buffer_walk, numnodepairs, debug, True, Gexisting)\n",
    "        #output_MST, cov_MST = calculate_metrics_custom(res[\"MST\"], res[\"MST_abstract\"], G_carall, nnids, output, buffer_walk, numnodepairs, debug, True, ig.Graph(), Polygon(), False, Gexisting)\n",
    "\n",
    "        output, covs = calculate_metrics_additively(res[\"GTs\"], res[\"GT_abstracts\"], res[\"prune_quantiles\"], G_carall, nnids, buffer_walk, numnodepairs, debug, True, Gexisting)\n",
    "        output_MST, cov_MST = calculate_metrics(res[\"MST\"], res[\"MST_abstract\"], G_carall, nnids, output, buffer_walk, numnodepairs, debug, True, ig.Graph(), Polygon(), False, Gexisting)\n",
    "\n",
    "        # Save the covers\n",
    "        write_result(covs, \"pickle\", placeid, poi_source, prune_measure,  \"_\"+ attr+\"_covers.pickle\")\n",
    "    #     write_result(covs_carminusbike, \"pickle\", placeid, poi_source, prune_measure, \"_covers_carminusbike.pickle\")\n",
    "        write_result(cov_MST, \"pickle\", placeid, poi_source, prune_measure, \"_\"+ attr+\"_cover_mst.pickle\")\n",
    "\n",
    "        # Write to CSV\n",
    "        write_result(output, \"dict\", placeid, poi_source, prune_measure, \"_\"+ attr+\".csv\")\n",
    "    #     write_result(output_carminusbike, \"dict\", placeid, poi_source, prune_measure, \"_carminusbike.csv\")\n",
    "    #     write_result(output_carconstrictedbike, \"dict\", placeid, poi_source, prune_measure, \"_carconstrictedbike.csv\")\n",
    "        write_result(output_MST, \"dict\", placeid, poi_source, \"\",  attr+\"_mst.csv\")\n",
    "        covs.clear()\n",
    "        output.clear()\n",
    "        output_MST.clear()\n",
    "        Gexisting.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OSMNX",
   "language": "python",
   "name": "osmnx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
